1/19/15

Overview of sequencing data
	old:
	Maxam-Gilbert sequencing
	Sanger sequencing - dominated for 30 years, but doesn't really scale up to do more than 1 sequence at a time
	
	2nd generation sequencing:
	Illumina hi-seq
	IonTorrent (smaller scale than illumina) - an improved version of 454 data
		do many fragments in parallel!
		increases throughput
	
	2nd gen method: 	fragment DNA
						ligate adapters of known sequence
						bind (in illumina) to a glass flow cell - create clusters
						add nucleotides
						they stick
						wash it
						take a picture and then see what base pair was added
						cleave it off
						repeat add/wash/picture
						
	sequencing types:
		whole genome sequencing
			does the ENTIRE genome
			involved, especially for bigger genomes
			lots of processes to get the data
		sequence capture
			attach complementary RNA or DNA strands to beads
			fish out the desired sequences with magnet
			create sequencing library from enriched DNA
			reduces cost and analysis time
		RNAseq
			extract DNA and convert to cDNA
			permits the analysis of expression and sequence variation in expressed genes (at the same time)
		RadSeq/GBS (genotyping by sequencing)
			reduce genome complexity with REs
			sequence highly multiplexed libraries of restriction fragments
			simultaneous marker discovery and genotyping
				i.e. lets you discover markers at the same time as analyzing them
				but can have higher error rates
		Amplicon sequencing
			use PCR to amplify target DNA
			sequence amplified DNA (Amplicon)
			
	Disadvantages of 2nd gen sequencing
		DNA must be amplified to create libraries
			even the best enzymes have error, so that is introduced here
		some fragments inevitably amplify better than others
			overrepresentation and underrepresentation happens
		so it'd be better if we didn't have to do any DNA amplification
		
		de novo assembly is difficult because of things like repeats
			hard to assemble because our fragments are too short to span long sections of repeats and know how things align
		short reads can miss structural variants in the genome, e.g. inversion or translocation
			can try to fix this with higher levels of read depth
			and need to not have repeated sequences around the break point
		short reads also make it hard to detect small insertions and deletions
		and short reads are challenging to align
		
	SO we need longer reads
	
	3rd gen sequencing is supposed to fix read length and the amplification problem
		sample prep less complicated
			no amplification, sequence DNA directly
		but need very high quality DNA
		
		2 types:
		sequencing by synthesis
			PacBio 
			attach polymerase directly and WATCH IN REAL TIME as DNA is sequenced
			detects fluorescence of base at polymerase
		direct sequencing by passing DNA through a nanopore (membrane-bound)  (oxford nanopore)
			ion flow changes at the pore each time a base passes through
			further behind than PacBio but could be easier and cheaper
		
		these both can have high error rates though  (10-25%)
		but can get MUCH longer reads!! fewer contigs are longer and easier to assemble
		still more expensive though
		
		
1/20/15

FastQ files and quality checking and trimming
	FastA sequence and quality scores stored in separate files (.fasta or .fa and .qual)
	now mainly used for storing reference sequences as either nucleotides or peptides
	2 lines per sequence read	
		quality means confidence in that sequence score
		sometimes the data is split into multiple lines, sometimes not
		always begins with an ">"
	
	FastQ sequence and quality scores stored in the same file (usually .fq or .fastq)
	most common format for short read data returned from the sequencer
	4 lines per sequence read
		always begins with an "@"
		line 1 is header, containing sequence identifier (sequencer, lane, location, info, etc)
		second line is sequence
		3rd line is spacer
		4th line is quality score
	
	historically 2 formats for quality scores
		sanger:
			quality (Q) = -10 times log 10 of p where p is the probability that given base call is incorrect
			e.g. Q30 means a 0.1% chance that the base call is incorrect
			Q20 is 1% chance incorrect, and Q10 is 10% chance incorrect
			quality score increases as you have low probabilities of being incorrect
		solexa:
			the other type uses same thing but times p/(1-p)
		these two values differ more the more incorrect you get (in terms of base pair probability of being wrong)
		
		it is possible to recalibrate quality scores to be really sure about how good the reads are, but won't go into that too much in this class
	
	Q scores are encoded by different characters (ASCII characters)
	now everyone uses the illumina 1.8 encoding, which uses the same quality score coding system as the original Sanger coding
	
	Preparing FASTQ files for analysis
		check files for completeness, use md5 checksums if file corruption is suspect
		inspect quality statistics
		many other possible steps to clean files, and the choice of steps depends on the application
			de-multiplexing
			trimming adapters
			filtering low quality base calls
			removing contaminant sequences
			removing duplicate sequences
			removing sequences that are mainly adapter
		there are many programs to implement these steps
		the first 2 are usu done by the sequencing center
		the first 3 are usually for genotyping and RNAseq
		all of them are usually done for reference assembly
		
	inspecting quality statistics
		number and length of sequences
		base qualities
		polyA/T tails
		sequence complexity
		presence of tag sequences (stuff you added during preparation)
		
		Recommended tools:
			prinseq
			fastqc
	
	one way to look at things is to look at the distribution of base frequencies across the sequence read
	
	quality is usu low in the beginning of the read, increases and then drops off even more as you go further along
		can tail off much more quickly if the sequencer didn't work well
	
	multiplexing is when several libraries are barcoded and sequenced on the same lane (of illumina)
		most sequencing centers will de-multiplex the data prior to returning it
		otherwise, CASAVA can be used for de-multiplexing and trimming the barcodes
	
	trimming adapters
		adapters are short sequences that are added to the beginning and the end of DNA molecules to prepare them for sequencing
		failure to remove them will compromise how well the reads align to a reference
		adapters can be detected during quality control phase and then removed by a range of tools
		most sequencing centers will already remove these
		Tools: fastx toolkit, trimmomatic, prinseq
	
	filtering low quality base calls
		choice of quality score to filter depends on the application
			too low quality score cutoff and bad sequence data wil be included
				may not be a problem for alignment and genotype calling, but it can slow things down
				more of a problem for assembly
			too high a quality score cutoff will results in loss of useful data
			usually Q10-Q20 is used, but sometimes higher or lower
			recommended tools: fastxtoolkit, trimmomatic, prinseq
	
	additional filtering steps can be done for assembly
		remove sequences consisting of adapter dimers (otherwise they may be included as contigs) - use TAGDUST
		clean out contaminants by blasting to known databases (can also be conducted post-assembly)
		remove duplicate sequences - if you are doing a de novo assembly, sequences that are exact copies will slow down the assembly without adding anything ( remove using FASTX_COLLAPSER)
		
		SnoWhite is a good pipeline for aggressive cleaning and filtering prior to assembly
		
	Re-pairing reads after filtering
		with paired-end reads if one read direction is removed but the other is not, then the _R1 and _R2 files are mismatched
		need to run a script to eliminate unpaired reads from each _R1 and _R2 file
		some programs output reads in paired and unpaired files (e.g. prinseq, Trimmomatic) others do not and custom scripts are required to re-pair data
		
	GBS-specific filtering
		GBS/RAD use enzymes to cleave the DNA, so all reads will begin with the recognition sequence
		cut the first X-bases from each sequence (all filtering programs)
		may need to clean GBS-specific adapters or other home-brew sequences that sequencing centers didn't remove
		
	RESOURCES: Del Fabbro et al 2013, an extensive evaluation of read trimming effects on illumina NGS data analysis. PLoS One 8:e85024
	prinseq.sourceforge.net/manual.html#STANDALONE and /Data_preprocessing.pdf
	
EXERCISES
	for GBS files, same number of sequences, diff. number of bases, diff mean length of input
		all bases are good quality - HOW DOES IT KNOW?
		

		
1/21/15

Alignment algorithms and tools
	when we align sequences, we assume they are similar
		protein sequence conservatio nand DNA sequence conservation are assumptions
	
	orthologous sequences differ because they are found in diff. species
	paralogous sequences differ due to a gene duplication event
	homoeologous sequences originated by polyploidization
		sequences could be all 3 or a combination
	
	pairwise alignments compares 2 sequences at a time
	multiple alignments compare more than 2 at a time and are more intensive
	
	pairwise: 2 challenges:
		there are many possible alignments
		large database/reference
	*2 sequences can ALWAYS be aligned
	*there is often more than one solution with the same score
	
	Methods:
		by hand - slide sequences on 2 lines of a word processot
		dot plot (with windows)
		mathematical approach (relatively slow, but guarantees finding optimal solution
		heuristic methods (fast, aproximate)
			BLAST
			short-read aligners
		
		need a scoring method for aligning, e.g 1 pt for a match, -1 for a mismatch, maybe -2 for a gap
		
		in a dot plot, each diagonal corresponds to a possible (ungapped) alignment -- x-axis is a sequence and y-axis is a sequence
			can increase "word size" and stringency
			or can set a score within a given window size
		
			smaller window means larger weight of statistical (unspecific) matches
			large windows reduce the sensitivity for short sequences
			insertions/deletions are not treated explicitly
		
		can use dynamic programming
			take the alignment, break it down to subproblems, solve those locally, then track back over all the alignments to get the total solution and guarantees the optimal solution
				global alignments start at the beginning
				local alignment algorithms find the region or regions of highest similarity between 2 sequences
				
				global is slower
				local is faster- starts at a "Seed" and extends from the site
			
			build an alignment path matrix
			create
			...
			
			scoring system
				numerical value based on a symbol comparison table
				gap penalties - either for opening a gap or for elongating a gap
			
			usu the best alignment has minimized the number of gaps and maximize the matches, but there is often a tradeoff
				how to balance this?
					steep penalty for a gap or else you end up with nonsense alignments
					"affine" gap penalty give a big penalty for each new gap but a much smaller gap extension
					multibase gaps are quite common
			
			however what to do with insertions??
				well there is still a penalty, but can get a higher score if the gap creates more alignments downstream
		
		BLAST
			very fast
			primarily designed to identify homologous sequences
			blast is a hashed seed-extend algorithm
			...
			...
			
		once it makes a match, it alignes from either end until it reaches an alignment score falling below 50%
		
	there are LOTS of aligners now
	
	aligning short sequences to a reference
		don't use blast because it is slow b/c it searches for each 1
		...
		...
		...
		...		
	
	short read aligning is hard b/c they are so short- contain limited information
		so harder to find a unique position on the reference genome
	better approaches: spaced seeds OR burrows-wheeler
		burrows wheeler does a compression that helps to speed things up
	hashed seed
		2 steps
			identify a match
			...
		
		with SNPs in a sequence, that is a mismatch and can make it fail to align
		spaced seed approach helps this
			more than 55% more sensitive than BLAST's default 70%
		
	burrows-wheeler
		a family of methods uses a Trie strucure to search a reference sequence
		trie is a data structure which stores the suffixes (i.e. ends of a sequence)
		key advantage over hashed algorithms:
			alignment of multiple copies of an identical sequence in reference only needs to be done once
			use of an FM-index
			...
		
		bowtie/soap vs bwa
		bowtie and soap 2 cannot handle gapped alignments (bowtie 2 can)
			so no indel detection
		
	a single read may align to many regions on the genome - makes a worse score
		could be due to duplication or repetitive sequences
	aligners let you choose how to deal with this, sometimes choose one of the places to align to randomly - level of randomness can be biased across aligners
	
	PCR duplicates can cause a problem, could bias SNP calls or introduce false SNPs
	
	SAM/BAM format
		sequence alignment/map format
			universal standard
			human readable (SAM) and compact (BAM)
		structure:
			header - version, sort order, reference sequences, read groups, program...
			alignment records
		sam/bam flag - has information about how the alignment performed
		
		See:  https://broadinstitute.github.io/picard/explain-flags.html
		
		mapping quality is calculated similarly to the RAD/GBS sequencing lecture from previous days' notes
			P = probability that this mapping is NOT the correct one
			MapQ = Qs = -10log10(P)
			calculating P is more complicated, see slide
		CIGAR string format also gives information about the alignment
	
	
	
	
1/22/15

Assembly - transcriptome and genome assembly - algorithms and tools

	method to bring all the reads together to reconstruct the DNA
	in sized order: reads -> contigs -> scaffold -> assembly
	the assembly will have some gaps because it is made from the very small fragments
	
	size of the genome matters in terms of ability and strategies to assemble
	
	assembling a draft vs a complete genome
		500 contigs covering most of a bacterial genome can be obtained quite quickly
		to get 1 contig covering all genomic sequence could take much longer
		usually that extra effort is not worth it, but people still sometimes want to do it
		
	how much coverage is needed? how many gaps will there be for a given number of reads and length of reads
	
	reference-guided assembly means using a reference genome
	de novo assembly is from nothing, and has 2 ways:
		overlap layout consensus (OLC)
		de Bruijn Graph (DBG)
	or a hybrid approach ......
	
	use graph theory to find the shortest or simplest way to assemble all the reads
		given a set of strings, find the shortest string that contains all of them
	
	scaffolding is the task of ordering and orienting contigs by using additional information about their relative placement
		mate-reads information, homology data, ..... is used to inform this
	
	mate-pair vs paired-end
		paired end usu refers to libraries prepared for the illumina platform with insert sizes 50-500bp and forward-reverse orientation
		mate-pair is a diff. library prep .......
		
1/23/15

RNAseq and analysis of differential gene expression

	why RNAseq?
	3 main uses
		assembling the gene space of a genome (a transcriptome)
		genotyping individuals for variants that occur within the transcribed region of their genome
		quantify patterns of gene expression across:
			organ, tissue or cell types
			timepoints and development
			experimental treatments or observational categories
		
	how is RNAseq data generated?
		mRNA is isolated, fragmented, and cDNA synthesized and sequenced
		RNA's with poly-A tails are captured, this gets only the mature mRNAs
		break into fragments
		create cDNA library (copy of the RNA?)
		amplify that cDNA
	
	want to do aggressive filtering prior to transcriptome assemblies - SnoWhite is good
	when doing mapping/aligning, you can be less aggressive with filtering, but bad quality reads won't align
	
	mapping = placement of a read in the correct region of the reference
	alignment = detailed placement of each base in a read - which includes mapping
	
	RNA has different problems when trying to align to a reference
		challenge 1 - mapping reads across intron-exon boundaries
			solutions:	map reads to a transcriptome (e.g. RSEM)
						two-stage mapping to the genome (e.g. TopHat)
						seed extension methods map small chunks to the genome and extend to junctions, e.g. GSNAP
		challenge 2 - splice variants - differential expression of alternatively spliced transcripts
				can look at quantities of various isoforms (versions of splicing) to see what's happening
			"challenge 2 = identifying abundance of alternatively spliced variants"
		challenge 3 - map filtered reads back to the transcriptome and count the number of reads that align to each contig
			reads can map to your contig or a paralagous read in a different gene (reads that have a high map score to the genome) - called multi-reads
				due to gene duplications etc
			"challenge 3 = dealing with multireads at the gene- and isoform-level"
			solutions:	discard them and estimate expression only from uniquely mapping reads
						rescue multiireads by allocating fractions of them to each contig, in proportion to the number of uniquely mapping reads mapping to each contig
						max-likelihood algorithms to assign multireads and sum across all isoforms to get a gene-level estimate, e.g. RSEM
	
	a practical approach to quantifying expression
		RSEM, provides a single pipeline to align sequence reads and eatimate expression counts for each contig
		when used in conjunction with Trinity, a trinity-build transcriptome assembly, it will estimate isoform-level expression counts
		in contrast to tophat and other approaches, it does not require a sequenced genome already assembled, and reasonable ref. transcriptomes can be built de novo using trinity non-model organisms
		.....
	
		tophats and cufflinks
			use a reference genome to quantify expression levels
			tophat may be less accurate than RSEM
	
	last step - counting the number of reads to quantify expression
		i.e. we have to normalize because there are lots of read fragments, and it varies by length of the gene we were using and the difference in individuals in terms of the total amount of RNA analyzed
		FPKM is one method - fragments per kilobase of transcript per million reads mapped --- standardize per gene length and number of reads per individual that were mapped anywhere on the genome
			this allows comparable comparison across genes and individuals
		RPKM is similar, reads per kilobase of transcript per million reads mapped
		
	in the end we find out amount of mRNA represented by the reads we have
	
	then statistical analysis of differences in expression vs background noise
	
	*technical replication means multiple libraries per individual to get an estimate of variance among sequencing runs
	
	2 broad approaches to analysis:
		1) analyzing differential gene expression on gene-by-gene basis, e.g. DESeq, EdgeR, limma
			examine how each gene is affected by a given treatment
			...
		2) analysis of patterns of gene co-expression and identification of clusters of genes that have similar patterns of expression
			identify clustes of genes that are upregulates in treatment X and downregulated in treatment Y
		
	
	2 sources of variation in RNAseq
		biological variation - the stuff we care about - differences in expression arising due to real differences
			any uncontrolled errors should be homogeneous across inds
		technical variation - measurement error inherent in the sequencing process
			if we could sequence an infinite number of reads, technical error would disappear
	
	analysis: do a regression of the normalized counts onto the variables of interest
	
	not all genes are expected to behave the same though, e.g. important genes will be constrained in constant expression, whereas a less important one may have no constraints and much more variance in expression
		EdgeR tries to work with both of these assumptions
		fits a common dispersion model, then a tagwise dispersion model - leads to more accurate estimate of if there is differential expression in genes
	
	heat maps are also a good way to display gene expression
	
	there are lots of technical considerations with RNAseq
		especially depth of coverage
			highly dependent on study organism and transcriptome size
			little power to detect changes in expression when <50 counts per million per gene, so try to have 50 counts per million per gene or more
			too many individuals per lane can increase your technical variation
	

1/26/15

Variant calling

	step 1 - alignments
		need to remember which tool was used for aligning so that quality information is known as well
	
	after aligning, you get the cigar string which has all the mismatches
		so these are all the variants we want
		BUT have to remove the false positives
	
		can either find this yourself or using Picard
		if 2 strings have the same starting position and the same cigar string, then they are duplicates
		
	step 2(?) remove duplicates?
	step 3(?) realignments - around indels
	
	any sequence mismatch = an error EXCEPT for known variants
	
	if both alleles exist on both strands, then it is probably real?
	
	Beagle is an imputation software - ADAPTREE DIDN'T USE THIS FOR SOME REASON
	
	VCF files store variant information
	
	there are tools to detect structural variation as well as copy number variation
	
	exercises - call variants, check vcf, prepare snp table
	
	
1/27/15

Genome scans and signatures of demography and adaptation
	
	genomic data gives greater resolution to evaluate and illustrate patterns
	more power as well with genomic data, so new methods to do more fine scale or detailed things?
	
	
	bayes factors interpreted much more vaguely than p values?
	so instead use relative significance - an empirical p value based on a distribution of loci known to be neutral
		is that distribution shifted from your test distribution?
		- problem is we don't know for sure if something is neutral
		- another problem is false positives
		